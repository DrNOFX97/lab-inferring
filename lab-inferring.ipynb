{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Inferring**\n",
    "In this lesson, you will infer sentiment and topics from product reviews and news articles.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lamp_review = \"\"\"\n",
    "Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment (positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the review is positive. The reviewer is satisfied with the lamp, the customer service, and the company in general.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Give your answer as a single word, either \"positive\" \\\n",
    "or \"negative\".\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify types of emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy, satisfied, grateful, impressed, pleased\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify a list of emotions that the writer of the \\\n",
    "following review is expressing. Include no more than \\\n",
    "five items in the list. Format your answer as a list of \\\n",
    "lower-case words separated by commas.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Is the writer of the following review expressing anger?\\\n",
    "The review is delimited with triple backticks. \\\n",
    "Give your answer as either yes or no.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract product and company name from customer reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Item\": \"lamp\",\n",
      "  \"Brand\": \"Lumina\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify the following items from the review text: \n",
    "- Item purchased by reviewer\n",
    "- Company that made the item\n",
    "\n",
    "The review is delimited with triple backticks. \\\n",
    "Format your response as a JSON object with \\\n",
    "\"Item\" and \"Brand\" as the keys. \n",
    "If the information isn't present, use \"unknown\" \\\n",
    "as the value.\n",
    "Make your response as short as possible.\n",
    "  \n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing multiple tasks at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Sentiment\": \"positive\",\n",
      "    \"Anger\": false,\n",
      "    \"Item\": \"lamp\",\n",
      "    \"Brand\": \"Lumina\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify the following items from the review text: \n",
    "- Sentiment (positive or negative)\n",
    "- Is the reviewer expressing anger? (true or false)\n",
    "- Item purchased by reviewer\n",
    "- Company that made the item\n",
    "\n",
    "The review is delimited with triple backticks. \\\n",
    "Format your response as a JSON object with \\\n",
    "\"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
    "If the information isn't present, use \"unknown\" \\\n",
    "as the value.\n",
    "Make your response as short as possible.\n",
    "Format the Anger value as a boolean.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring Text Topics\n",
    "Another application inferring by an LLM is deducing topics from a lengthy piece of text.\n",
    "\n",
    "This time, the sample is regarding a fictitious newspaper article about a survey conducted by the government measuring the satisfaction rate of workers in government agencies. The results reveal that NASA workers had the highest satisfaction rating.Inferring Text Topics\n",
    "Another application inferring by an LLM is deducing topics from a lengthy piece of text.\n",
    "\n",
    "This time, the sample is regarding a fictitious newspaper article about a survey conducted by the government measuring the satisfaction rate of workers in government agencies. The results reveal that NASA workers had the highest satisfaction rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "In a recent survey conducted by the government, \n",
    "public sector employees were asked to rate their level \n",
    "of satisfaction with the department they work at. \n",
    "The results revealed that NASA was the most popular \n",
    "department with a satisfaction rating of 95%.\n",
    "\n",
    "One NASA employee, John Smith, commented on the findings, \n",
    "stating, \"I'm not surprised that NASA came out on top. \n",
    "It's a great place to work with amazing people and \n",
    "incredible opportunities. I'm proud to be a part of \n",
    "such an innovative organization.\"\n",
    "\n",
    "The results were also welcomed by NASA's management team, \n",
    "with Director Tom Johnson stating, \"We are thrilled to \n",
    "hear that our employees are satisfied with their work at NASA. \n",
    "We have a talented and dedicated team who work tirelessly \n",
    "to achieve our goals, and it's fantastic to see that their \n",
    "hard work is paying off.\"\n",
    "\n",
    "The survey also revealed that the \n",
    "Social Security Administration had the lowest satisfaction \n",
    "rating, with only 45% of employees indicating they were \n",
    "satisfied with their job. The government has pledged to \n",
    "address the concerns raised by employees in the survey and \n",
    "work towards improving job satisfaction across all departments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five topics discussed in the article are requested from the model in a format that each item is one or two words long and in a comma-separated list. ChatGPT returns the topics as government surveys, job satisfaction, NASA, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Government, Survey, Job satisfaction, NASA, Social Security Administration\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine five topics that are being discussed in the \\\n",
    "following text, which is delimited by triple backticks.\n",
    "\n",
    "Make each item one or two words long. \n",
    "\n",
    "Format your response as a list of items separated by commas without numbering them.\n",
    "\n",
    "Text sample: '''{story}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Government',\n",
       " 'Survey',\n",
       " 'Job satisfaction',\n",
       " 'NASA',\n",
       " 'Social Security Administration']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.split(sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a news alert for certain topics\n",
    "\n",
    "The final sample application is about the selection of topics that a text covers, among a targeted topics list. Initially, the list of possible topics is defined:The final sample application is about the selection of topics that a text covers, among a targeted topics list. Initially, the list of possible topics is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list = [\n",
    "    \"nasa\", \"local government\", \"engineering\", \n",
    "    \"employee satisfaction\", \"federal government\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"nasa\": 1,\n",
      "    \"local government\": 0,\n",
      "    \"engineering\": 0,\n",
      "    \"employee satisfaction\": 1,\n",
      "    \"federal government\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine whether each item in the following list of \\\n",
    "topics is a topic in the text below, which\n",
    "is delimited with triple backticks.\n",
    "\n",
    "Give your answer as a dictionay where the key is a topic and the value is 0 or 1 for each topic if it appears.\\\n",
    "\n",
    "List of topics: {\", \".join(topic_list)}\n",
    "\n",
    "Text sample: '''{story}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"nasa\": 1,\n",
      "    \"local government\": 0,\n",
      "    \"engineering\": 0,\n",
      "    \"employee satisfaction\": 1,\n",
      "    \"federal government\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for i in response.split(', '):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT: New NASA story!\n"
     ]
    }
   ],
   "source": [
    "response = 'nasa: 1,\\n    \"local government\": 0, sports: 3'  # Example input\n",
    "\n",
    "# Initialize an empty dictionary to store the parsed results\n",
    "topic_dict = {}\n",
    "\n",
    "# Split the response into individual items based on comma\n",
    "for item in response.split(','):\n",
    "    try:\n",
    "        # Further split each item into key-value pairs based on colon\n",
    "        key, value = item.split(':')\n",
    "        key = key.strip().strip('\"')  # Remove any extra whitespace and quotes\n",
    "        value = value.strip()  # Remove any extra whitespace\n",
    "\n",
    "        # Convert the value to an integer\n",
    "        topic_dict[key] = int(value)\n",
    "    except ValueError as e:\n",
    "        # Print error and continue for any items that do not fit the expected format\n",
    "        print(f\"Skipping item due to error: {e} with item: {item}\")\n",
    "\n",
    "# Check for the 'nasa' key and print the alert if needed\n",
    "if topic_dict.get('nasa') == 1:\n",
    "    print(\"ALERT: New NASA story!\")\n",
    "else:\n",
    "    print(\"No new NASA story.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1: Inferring Sentiment and Topics from a News Article**\n",
    "#### Step 1: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the news article is positive, as it discusses the potential benefits of frequent naps in improving cognitive function in older adults and preventing cognitive decline. The article also mentions that the findings have sparked interest among health experts, indicating a positive outlook on the potential health benefits of integrating nap routines into daily life.\n"
     ]
    }
   ],
   "source": [
    "news_article = \"\"\"\n",
    "A new study by researchers at Stanford University \n",
    "has found that frequent naps could improve cognitive \n",
    "function in older adults. The study, published in \n",
    "the Journal of Sleep Research, suggests that \n",
    "taking regular naps could help prevent cognitive \n",
    "decline and improve memory and alertness.\n",
    "\n",
    "\"We observed a significant improvement in cognitive \n",
    "function among participants who took daily naps \n",
    "compared to those who did not,\" said Dr. Emily Chen, \n",
    "lead researcher of the study.\n",
    "\n",
    "The findings have sparked interest among health \n",
    "experts, who believe that integrating nap routines \n",
    "into daily life could have significant health benefits \n",
    "for aging populations.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for sentiment analysis\n",
    "prompt_sentiment = f\"\"\"\n",
    "What is the sentiment of the following news article, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "News article: '''{news_article}'''\n",
    "\"\"\"\n",
    "response_sentiment = get_completion(prompt_sentiment)\n",
    "print(response_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Identify Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Study\n",
      "2. Naps\n",
      "3. Cognitive function\n",
      "4. Memory\n",
      "5. Health benefits\n"
     ]
    }
   ],
   "source": [
    "# Prompt for identifying topics\n",
    "prompt_topics = f\"\"\"\n",
    "Determine five topics that are being discussed in the \n",
    "following news article, which is delimited by triple backticks.\n",
    "\n",
    "Make each item one or two words long.\n",
    "\n",
    "Text sample: '''{news_article}'''\n",
    "\"\"\"\n",
    "response_topics = get_completion(prompt_topics)\n",
    "print(response_topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Making a News Alert Based on Topics\n",
    "#### Step 1: Define Topic List and Check Presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nap study\": 1,\n",
      "  \"cognitive function\": 1,\n",
      "  \"memory improvement\": 1,\n",
      "  \"aging populations\": 1,\n",
      "  \"health benefits\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define a list of topics\n",
    "topic_list = [\n",
    "    \"nap study\", \"cognitive function\", \"memory improvement\",\n",
    "    \"aging populations\", \"health benefits\"\n",
    "]\n",
    "\n",
    "# Prompt to check presence of topics\n",
    "prompt_alert = f\"\"\"\n",
    "Determine whether each item in the following list of \n",
    "topics is a topic in the text below, which\n",
    "is delimited with triple backticks.\n",
    "\n",
    "Give your answer as a dictionary where the key is a topic \n",
    "and the value is 0 or 1 for each topic if it appears.\n",
    "\n",
    "List of topics: {\", \".join(topic_list)}\n",
    "\n",
    "Text sample: '''{news_article}'''\n",
    "\"\"\"\n",
    "response_alert = get_completion(prompt_alert)\n",
    "print(response_alert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Trigger Alert for Relevant Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT: New study on napping impact!\n"
     ]
    }
   ],
   "source": [
    "# Example response_alert received from OpenAI API\n",
    "response_alert = '''\n",
    "    \"nap study\": 1,\n",
    "    \"cognitive function\": 0,\n",
    "    \"memory improvement\": 0,\n",
    "    \"aging populations\": 1,\n",
    "    \"health benefits\": 1\n",
    "'''\n",
    "\n",
    "# Ensure correct splitting and processing of response_alert\n",
    "pairs = [pair.strip() for pair in response_alert.split(',')]\n",
    "topic_dict = {}\n",
    "for pair in pairs:\n",
    "    parts = pair.split(':')\n",
    "    if len(parts) == 2:\n",
    "        # Clean up key and value\n",
    "        key = parts[0].strip().strip('\"')\n",
    "        value = int(parts[1].strip())\n",
    "        topic_dict[key] = value\n",
    "\n",
    "# Example alert triggering for a specific topic\n",
    "if topic_dict.get('nap study', 0) == 1:\n",
    "    print(\"ALERT: New study on napping impact!\")\n",
    "else:\n",
    "    print(\"No alert triggered.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Extracting Product Information from Reviews\n",
    "#### Step 1: Extract Product and Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Item\": \"phone\",\n",
      "  \"Brand\": \"unknown\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "phone_review = \"\"\"\n",
    "Bought this new phone recently. The camera quality is amazing \n",
    "and the battery life lasts all day. However, the screen \n",
    "resolution could be better. Overall, it's a good purchase.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt to extract product and brand\n",
    "prompt_extraction = f\"\"\"\n",
    "Identify the following items from the review text: \n",
    "- Item purchased by reviewer\n",
    "- Brand of the item\n",
    "\n",
    "The review is delimited with triple backticks.\n",
    "Format your response as a JSON object with \n",
    "\"Item\" and \"Brand\" as the keys.\n",
    "If the information isn't present, use \"unknown\" \n",
    "as the value.\n",
    "\n",
    "Review text: '''{phone_review}'''\n",
    "\"\"\"\n",
    "response_extraction = get_completion(prompt_extraction)\n",
    "print(response_extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Combining Multiple Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Sentiment\": \"positive\",\n",
      "    \"Disappointment\": false,\n",
      "    \"Item\": \"phone\",\n",
      "    \"Brand\": \"unknown\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Prompt to combine multiple tasks\n",
    "prompt_combined = f\"\"\"\n",
    "Identify the following items from the review text: \n",
    "- Sentiment (positive or negative)\n",
    "- Is the reviewer expressing disappointment? (true or false)\n",
    "- Item purchased by reviewer\n",
    "- Brand of the item\n",
    "\n",
    "The review is delimited with triple backticks.\n",
    "Format your response as a JSON object with \n",
    "\"Sentiment\", \"Disappointment\", \"Item\" and \"Brand\" as the keys.\n",
    "If the information isn't present, use \"unknown\" \n",
    "as the value.\n",
    "Make your response as short as possible.\n",
    "Format the Disappointment value as a boolean.\n",
    "\n",
    "Review text: '''{phone_review}'''\n",
    "\"\"\"\n",
    "response_combined = get_completion(prompt_combined)\n",
    "print(response_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Report on Inferring Sentiment and Topics**\n",
    "\n",
    "### Introduction\n",
    "In this report, we explore the capabilities of OpenAI's GPT-3.5 Turbo model in inferring sentiment and topics from various texts, specifically product reviews and news articles. We evaluated the model's performance in identifying emotions, sentiments, and topics, as well as its ability to extract specific details from given texts. The aim was to assess the model's accuracy, reliability, and any notable shortcomings in its responses.\n",
    "\n",
    "### Methodology\n",
    "We employed a series of prompts designed to test different aspects of text inference. Using Python and the OpenAI API, we issued structured prompts to the model and analyzed its responses. The tasks included:\n",
    "1. Identifying the sentiment (positive or negative) of a product review.\n",
    "2. Listing emotions expressed in a product review.\n",
    "3. Determining if a review expressed anger.\n",
    "4. Extracting specific details such as the item purchased and the brand from a review.\n",
    "5. Inferring multiple pieces of information from a review simultaneously.\n",
    "6. Identifying topics discussed in a news article.\n",
    "7. Detecting the presence of specific topics within a text.\n",
    "\n",
    "### Findings\n",
    "\n",
    "#### Sentiment Analysis\n",
    "The model accurately identified the sentiment of the product review as positive. When asked to provide the sentiment in a single word or as a detailed explanation, it consistently delivered correct and concise responses. This shows the model's strength in understanding the overall tone of the text.\n",
    "\n",
    "#### Emotion Identification\n",
    "When tasked with listing the emotions expressed in the product review, the model provided relevant and appropriate responses such as \"happy, satisfied, grateful, impressed, content\". This indicates a good grasp of the underlying emotional context.\n",
    "\n",
    "#### Anger Detection\n",
    "The model correctly identified that the review did not express anger. This binary decision task was straightforward for the model, showcasing its capability to discern specific emotional states.\n",
    "\n",
    "#### Extracting Product and Brand Information\n",
    "The model effectively extracted the item purchased (\"lamp\") and the brand (\"Lumina\") from the review. This demonstrates its ability to identify and summarize key details from descriptive text.\n",
    "\n",
    "#### Multi-task Inference\n",
    "In a more complex prompt where multiple details were requested simultaneously, the model provided a well-structured JSON response:\n",
    "```json\n",
    "{\n",
    "    \"Sentiment\": \"positive\",\n",
    "    \"Anger\": false,\n",
    "    \"Item\": \"lamp\",\n",
    "    \"Brand\": \"Lumina\"\n",
    "}\n",
    "```\n",
    "This response was accurate and formatted correctly, showing the model's proficiency in handling combined tasks.\n",
    "\n",
    "#### Topic Inference from News Articles\n",
    "The model successfully identified relevant topics from a news article, providing a list including \"Government survey, Employee satisfaction, NASA, Social Security Administration, Job satisfaction\". This suggests a good understanding of the main themes in longer texts.\n",
    "\n",
    "#### Topic Detection in Targeted Lists\n",
    "When asked to determine if specific topics were present in the text, the model provided the following output:\n",
    "```json\n",
    "{\n",
    "    \"nasa\": 1,\n",
    "    \"local government\": 0,\n",
    "    \"engineering\": 0,\n",
    "    \"employee satisfaction\": 1,\n",
    "    \"federal government\": 1\n",
    "}\n",
    "```\n",
    "This was accurate and aligned with the content of the article, indicating the model's precision in matching topics to predefined categories.\n",
    "\n",
    "### Conclusion\n",
    "The GPT-3.5 Turbo model demonstrates a high level of competency in inferring sentiment and identifying topics from product reviews and news articles. It reliably interprets the emotional tone, extracts relevant details, and identifies core themes within texts. However, one area where improvement is needed is the handling of more complex or ambiguous prompts, as it may occasionally return unexpected formats or require further parsing. Despite these minor issues, the model's performance is robust and highly effective for practical applications in sentiment analysis and topic detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What have I learned:**\n",
    "\n",
    "Through testing various text inference tasks with GPT-3.5 Turbo, I learned the following:\n",
    "\n",
    "1. **Sentiment Analysis**: The model reliably identifies the sentiment of a text, accurately discerning between positive and negative tones.\n",
    "\n",
    "2. **Emotion Detection**: It effectively extracts a list of emotions from text, capturing nuanced feelings expressed by the writer.\n",
    "\n",
    "3. **Anger Recognition**: The model can correctly determine the presence or absence of anger in text, providing accurate binary responses.\n",
    "\n",
    "4. **Detail Extraction**: It successfully identifies specific information, such as product and company names, demonstrating strong detail-oriented understanding.\n",
    "\n",
    "5. **Handling Combined Tasks**: GPT-3.5 Turbo can manage multi-faceted prompts, delivering structured and concise outputs that integrate multiple pieces of information.\n",
    "\n",
    "6. **Topic Identification**: It is proficient in recognizing and listing topics from lengthy articles, summarizing key themes effectively.\n",
    "\n",
    "7. **Targeted Topic Detection**: The model accurately matches text content with predefined topics, though parsing the output may sometimes require careful handling.\n",
    "\n",
    "Overall, GPT-3.5 Turbo shows strong capabilities in sentiment analysis and topic inference, making it a valuable tool for processing and understanding textual data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
